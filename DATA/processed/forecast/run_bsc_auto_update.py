#!/usr/bin/env python3
"""
BSC Forecast Auto Update System
===============================

Quy tr√¨nh t·ª± ƒë·ªông h√≥a ho√†n ch·ªânh ƒë·ªÉ c·∫≠p nh·∫≠t d·ªØ li·ªáu BSC Forecast:
1. ƒê·ªçc file Excel BSC m·ªõi nh·∫•t
2. X·ª≠ l√Ω v√† chu·∫©n h√≥a d·ªØ li·ªáu  
3. T·∫°o file CSV chu·∫©n cho Streamlit
4. Backup v√† validation
5. Th√¥ng b√°o k·∫øt qu·∫£

Usage:
    python3 run_bsc_auto_update.py [--source SOURCE_FILE] [--force]

Author: AI Assistant
Date: 2025-10-08
"""

import sys
import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import shutil
import logging
import argparse
import json
from typing import Dict, List, Optional, Tuple

# Find project root (stock_dashboard directory)
def find_project_root() -> Path:
    """T√¨m project root b·∫±ng c√°ch t√¨m th∆∞ m·ª•c ch·ª©a stock_dashboard"""
    current = Path(__file__).resolve()
    while current.parent != current:
        if current.name == 'stock_dashboard':
            return current
        current = current.parent
    # Fallback: gi·∫£ s·ª≠ script ch·∫°y t·ª´ project root
    return Path(__file__).resolve().parent.parent.parent

PROJECT_ROOT = find_project_root()

# Setup logging v·ªõi absolute path
log_dir = PROJECT_ROOT / 'calculated_results' / 'forecast' / 'bsc'
log_dir.mkdir(parents=True, exist_ok=True)
log_file = log_dir / 'processing_log.txt'

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(str(log_file), mode='a'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class BSCAutoProcessor:
    """
    BSC Forecast Auto Processing System
    T·ª± ƒë·ªông h√≥a quy tr√¨nh c·∫≠p nh·∫≠t d·ªØ li·ªáu BSC
    """
    
    def __init__(self, config_path: str = "data_processor/forecast/bsc_config.json"):
        """
        Initialize BSC Auto Processor
        
        Args:
            config_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file c·∫•u h√¨nh
        """
        # Find project root
        self.project_root = find_project_root()
        
        # Resolve config path relative to project root
        if not os.path.isabs(config_path):
            self.config_path = self.project_root / config_path
        else:
            self.config_path = Path(config_path)
        
        self.config = self.load_config()
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.date_str = datetime.now().strftime("%Y%m%d")
        
        # ƒê∆∞·ªùng d·∫´n c√°c file (resolve relative to project root)
        input_file_rel = self.config['input']['excel_file']
        if not os.path.isabs(input_file_rel):
            self.input_file = self.project_root / input_file_rel
        else:
            self.input_file = Path(input_file_rel)
        
        output_dir_rel = self.config['output']['csv_dir']
        if not os.path.isabs(output_dir_rel):
            self.output_dir = self.project_root / output_dir_rel
        else:
            self.output_dir = Path(output_dir_rel)
        
        backup_dir_rel = self.config['output']['backup_dir']
        if not os.path.isabs(backup_dir_rel):
            self.backup_dir = self.project_root / backup_dir_rel
        else:
            self.backup_dir = Path(backup_dir_rel)
        
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        # File output ch√≠nh
        self.latest_csv = self.output_dir / "bsc_forecast_latest.csv"
        self.backup_csv = self.backup_dir / f"bsc_forecast_{self.date_str}.csv"
        
    def load_config(self) -> Dict:
        """Load c·∫•u h√¨nh t·ª´ file JSON"""
        default_config = {
            "input": {
                "excel_file": "data_processor/Bsc_forecast/BSC Master File Equity Pro.xlsm",
                "sheet_name": "Codedata",
                "fallback_file": "data_processor/Bsc_forecast/Database Forecast BSC.xlsx"
            },
            "output": {
                "csv_dir": "calculated_results/forecast/bsc",
                "backup_dir": "calculated_results/forecast/bsc/backup"
            },
            "columns": {
                "ticker": "symbol",
                "Rating": "rating", 
                "target_price": "target_price",
                "2025_rev": "rev_fy",
                "2026_rev": "rev_fy_1",
                "2025_npat": "npatmi_fy",
                "2026_npat": "npatmi_fy_1",
                "2025_roe": "roe_fy",
                "2026_roe": "roe_fy_1",
                "2025_roa": "roa_fy",
                "2026_roa": "roa_fy_1"
            },
            "validation": {
                "min_symbols": 50,
                "required_columns": ["symbol", "rating", "target_price", "rev_fy", "npatmi_fy"],
                "max_change_pct": 50
            }
        }
        
        if self.config_path.exists():
            try:
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                # Merge v·ªõi default config
                for key, value in default_config.items():
                    if key not in config:
                        config[key] = value
                    elif isinstance(value, dict):
                        for subkey, subvalue in value.items():
                            if subkey not in config[key]:
                                config[key][subkey] = subvalue
                return config
            except Exception as e:
                logger.warning(f"L·ªói ƒë·ªçc config, s·ª≠ d·ª•ng default: {e}")
        
        # T·∫°o file config m·∫∑c ƒë·ªãnh
        self.config_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.config_path, 'w', encoding='utf-8') as f:
            json.dump(default_config, f, indent=2, ensure_ascii=False)
        
        return default_config
    
    def backup_existing_csv(self) -> bool:
        """Backup file CSV hi·ªán t·∫°i n·∫øu c√≥"""
        try:
            if self.latest_csv.exists():
                # Copy file hi·ªán t·∫°i sang backup
                shutil.copy2(self.latest_csv, self.backup_csv)
                logger.info(f"‚úÖ ƒê√£ backup file c≈©: {self.backup_csv}")
                return True
            return False
        except Exception as e:
            logger.error(f"‚ùå L·ªói backup file: {e}")
            return False
    
    def read_excel_data(self, source_file: Optional[str] = None) -> pd.DataFrame:
        """
        ƒê·ªçc d·ªØ li·ªáu t·ª´ file Excel BSC
        
        Args:
            source_file: ƒê∆∞·ªùng d·∫´n file Excel (n·∫øu kh√¥ng d√πng default)
            
        Returns:
            DataFrame ch·ª©a d·ªØ li·ªáu BSC
        """
        try:
            # X√°c ƒë·ªãnh file input
            if source_file:
                excel_file = Path(source_file) if not os.path.isabs(source_file) else Path(source_file)
                if not excel_file.is_absolute():
                    excel_file = self.project_root / excel_file
            else:
                excel_file = self.input_file
            
            if not excel_file.exists():
                # Th·ª≠ file fallback
                fallback_file_rel = self.config['input']['fallback_file']
                if not os.path.isabs(fallback_file_rel):
                    fallback_file = self.project_root / fallback_file_rel
                else:
                    fallback_file = Path(fallback_file_rel)
                
                if fallback_file.exists():
                    logger.warning(f"File ch√≠nh kh√¥ng t·ªìn t·∫°i, s·ª≠ d·ª•ng fallback: {fallback_file}")
                    excel_file = fallback_file
                else:
                    raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file Excel: {excel_file}")
            
            logger.info(f"üìñ ƒê·ªçc d·ªØ li·ªáu t·ª´: {excel_file}")
            
            # ƒê·ªçc sheet v·ªõi header t·ª´ d√≤ng 7 (cho BSC Master File)
            sheet_name = self.config['input']['sheet_name']
            if 'BSC Forecast' in sheet_name:
                df = pd.read_excel(excel_file, sheet_name=sheet_name, header=7)
            else:
                df = pd.read_excel(excel_file, sheet_name=sheet_name)
            
            logger.info(f"‚úÖ ƒê√£ ƒë·ªçc {df.shape[0]} d√≤ng, {df.shape[1]} c·ªôt t·ª´ sheet '{sheet_name}'")
            
            return df
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói ƒë·ªçc file Excel: {e}")
            raise
    
    def process_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        X·ª≠ l√Ω v√† chu·∫©n h√≥a d·ªØ li·ªáu BSC
        
        Args:
            df: DataFrame g·ªëc t·ª´ Excel
            
        Returns:
            DataFrame ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a
        """
        try:
            logger.info("üîÑ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω d·ªØ li·ªáu...")
            
            # Copy ƒë·ªÉ kh√¥ng thay ƒë·ªïi d·ªØ li·ªáu g·ªëc
            processed_df = df.copy()
            
            # Chu·∫©n h√≥a t√™n c·ªôt
            processed_df.columns = processed_df.columns.str.strip()
            
            # Mapping c·ªôt theo c·∫•u h√¨nh
            column_mapping = self.config['columns']
            available_columns = [col for col in column_mapping.keys() if col in processed_df.columns]
            
            if not available_columns:
                raise ValueError("Kh√¥ng t√¨m th·∫•y c·ªôt n√†o ph√π h·ª£p trong d·ªØ li·ªáu")
            
            # L·∫•y ch·ªâ c√°c c·ªôt c·∫ßn thi·∫øt v√† ƒë·ªïi t√™n
            processed_df = processed_df[available_columns].copy()
            rename_mapping = {col: column_mapping[col] for col in available_columns}
            processed_df = processed_df.rename(columns=rename_mapping)
            
            # Chu·∫©n h√≥a symbol [[memory:8512150]]
            if 'symbol' in processed_df.columns:
                processed_df['symbol'] = processed_df['symbol'].str.upper().str.strip()
                # Lo·∫°i b·ªè c√°c d√≤ng c√≥ symbol r·ªóng
                processed_df = processed_df.dropna(subset=['symbol'])
                processed_df = processed_df[processed_df['symbol'] != '']
            
            # Chu·∫©n h√≥a rating
            if 'rating' in processed_df.columns:
                processed_df['rating'] = processed_df['rating'].str.upper().str.strip()
            
            # Chuy·ªÉn ƒë·ªïi ROE, ROA t·ª´ decimal sang percentage
            # Note: After config mapping, columns are already renamed to roe_2025, roe_2026, etc.
            for col in ['roe_2025', 'roe_2026', 'roa_2025', 'roa_2026']:
                if col in processed_df.columns:
                    processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce') * 100
            
            # Th√™m metadata
            processed_df['source'] = 'BSC'
            processed_df['update_date'] = datetime.now().strftime('%Y-%m-%d')
            processed_df['data_type'] = 'forecast'
            processed_df['processing_timestamp'] = self.timestamp
            
            # S·∫Øp x·∫øp theo symbol
            processed_df = processed_df.sort_values('symbol').reset_index(drop=True)
            
            logger.info(f"‚úÖ ƒê√£ x·ª≠ l√Ω {len(processed_df)} records v·ªõi {len(processed_df.columns)} c·ªôt")
            
            return processed_df
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói x·ª≠ l√Ω d·ªØ li·ªáu: {e}")
            raise
    
    def validate_data(self, df: pd.DataFrame, previous_df: Optional[pd.DataFrame] = None) -> Tuple[bool, List[str]]:
        """
        Validate ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu
        
        Args:
            df: DataFrame m·ªõi
            previous_df: DataFrame t·ª´ l·∫ßn c·∫≠p nh·∫≠t tr∆∞·ªõc (n·∫øu c√≥)
            
        Returns:
            Tuple (is_valid, warnings)
        """
        warnings = []
        is_valid = True
        
        try:
            logger.info("üîç Ki·ªÉm tra ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu...")
            
            validation_config = self.config['validation']
            
            # 1. Ki·ªÉm tra s·ªë l∆∞·ª£ng symbols
            min_symbols = validation_config['min_symbols']
            if len(df) < min_symbols:
                warnings.append(f"‚ö†Ô∏è Ch·ªâ c√≥ {len(df)} symbols, √≠t h∆°n m·ª©c t·ªëi thi·ªÉu {min_symbols}")
                is_valid = False
            
            # 2. Ki·ªÉm tra c√°c c·ªôt b·∫Øt bu·ªôc
            required_columns = validation_config['required_columns']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                warnings.append(f"‚ùå Thi·∫øu c√°c c·ªôt b·∫Øt bu·ªôc: {missing_columns}")
                is_valid = False
            
            # 3. Ki·ªÉm tra d·ªØ li·ªáu null trong c√°c c·ªôt quan tr·ªçng
            for col in required_columns:
                if col in df.columns:
                    null_count = df[col].isnull().sum()
                    null_pct = (null_count / len(df)) * 100
                    if null_pct > 20:  # H∆°n 20% null
                        warnings.append(f"‚ö†Ô∏è C·ªôt '{col}' c√≥ {null_pct:.1f}% gi√° tr·ªã null")
            
            # 4. So s√°nh v·ªõi d·ªØ li·ªáu tr∆∞·ªõc (n·∫øu c√≥)
            if previous_df is not None and not previous_df.empty:
                # Ki·ªÉm tra thay ƒë·ªïi s·ªë l∆∞·ª£ng symbols
                prev_count = len(previous_df)
                curr_count = len(df)
                change_pct = abs((curr_count - prev_count) / prev_count) * 100
                
                max_change = validation_config['max_change_pct']
                if change_pct > max_change:
                    warnings.append(f"‚ö†Ô∏è S·ªë l∆∞·ª£ng symbols thay ƒë·ªïi {change_pct:.1f}% (t·ª´ {prev_count} ‚Üí {curr_count})")
                
                # Ki·ªÉm tra symbols m·ªõi v√† m·∫•t
                if 'symbol' in df.columns and 'symbol' in previous_df.columns:
                    prev_symbols = set(previous_df['symbol'])
                    curr_symbols = set(df['symbol'])
                    
                    new_symbols = curr_symbols - prev_symbols
                    removed_symbols = prev_symbols - curr_symbols
                    
                    if new_symbols:
                        warnings.append(f"‚ûï Symbols m·ªõi: {sorted(list(new_symbols))}")
                    if removed_symbols:
                        warnings.append(f"‚ûñ Symbols b·ªã lo·∫°i: {sorted(list(removed_symbols))}")
            
            # 5. Ki·ªÉm tra gi√° tr·ªã b·∫•t th∆∞·ªùng
            if 'target_price' in df.columns:
                target_prices = pd.to_numeric(df['target_price'], errors='coerce')
                if target_prices.max() > 1000000:  # H∆°n 1 tri·ªáu VND
                    warnings.append("‚ö†Ô∏è C√≥ gi√° m·ª•c ti√™u b·∫•t th∆∞·ªùng (> 1M VND)")
                if target_prices.min() < 1000:  # D∆∞·ªõi 1000 VND
                    warnings.append("‚ö†Ô∏è C√≥ gi√° m·ª•c ti√™u b·∫•t th∆∞·ªùng (< 1K VND)")
            
            if warnings:
                logger.warning(f"Ph√°t hi·ªán {len(warnings)} c·∫£nh b√°o trong qu√° tr√¨nh validation")
                for warning in warnings:
                    logger.warning(warning)
            else:
                logger.info("‚úÖ D·ªØ li·ªáu ƒë√£ pass t·∫•t c·∫£ ki·ªÉm tra validation")
            
            return is_valid, warnings
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói trong qu√° tr√¨nh validation: {e}")
            return False, [f"‚ùå L·ªói validation: {e}"]
    
    def save_csv(self, df: pd.DataFrame) -> bool:
        """
        L∆∞u DataFrame th√†nh file CSV
        
        Args:
            df: DataFrame c·∫ßn l∆∞u
            
        Returns:
            True n·∫øu th√†nh c√¥ng
        """
        try:
            # L∆∞u file ch√≠nh
            df.to_csv(self.latest_csv, index=False, encoding='utf-8')
            logger.info(f"‚úÖ ƒê√£ l∆∞u file ch√≠nh: {self.latest_csv}")
            
            # L∆∞u file backup v·ªõi timestamp
            df.to_csv(self.backup_csv, index=False, encoding='utf-8')
            logger.info(f"‚úÖ ƒê√£ l∆∞u file backup: {self.backup_csv}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói l∆∞u file CSV: {e}")
            return False
    
    def load_previous_data(self) -> Optional[pd.DataFrame]:
        """Load d·ªØ li·ªáu t·ª´ l·∫ßn c·∫≠p nh·∫≠t tr∆∞·ªõc ƒë·ªÉ so s√°nh"""
        try:
            if self.latest_csv.exists():
                return pd.read_csv(self.latest_csv)
            return None
        except Exception as e:
            logger.warning(f"Kh√¥ng th·ªÉ load d·ªØ li·ªáu tr∆∞·ªõc: {e}")
            return None
    
    def create_processing_report(self, df: pd.DataFrame, warnings: List[str], processing_time: float) -> Dict:
        """T·∫°o b√°o c√°o qu√° tr√¨nh x·ª≠ l√Ω"""
        report = {
            "timestamp": self.timestamp,
            "processing_time_seconds": round(processing_time, 2),
            "input_file": str(self.input_file),
            "output_file": str(self.latest_csv),
            "records_processed": len(df),
            "columns_count": len(df.columns),
            "symbols_count": df['symbol'].nunique() if 'symbol' in df.columns else 0,
            "warnings": warnings,
            "success": len(warnings) == 0 or all('‚ùå' not in w for w in warnings)
        }
        
        # L∆∞u report
        report_file = self.output_dir / f"processing_report_{self.date_str}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        return report
    
    def run(self, source_file: Optional[str] = None, force: bool = False) -> bool:
        """
        Ch·∫°y to√†n b·ªô quy tr√¨nh auto update
        
        Args:
            source_file: File Excel ngu·ªìn (n·∫øu kh√¥ng d√πng default)
            force: B·ªè qua validation errors
            
        Returns:
            True n·∫øu th√†nh c√¥ng
        """
        start_time = datetime.now()
        
        try:
            logger.info("üöÄ B·∫ÆT ƒê·∫¶U QUY TR√åNH BSC AUTO UPDATE")
            logger.info("="*60)
            
            # 1. Backup file hi·ªán t·∫°i
            self.backup_existing_csv()
            
            # 2. Load d·ªØ li·ªáu tr∆∞·ªõc ƒë·ªÉ so s√°nh
            previous_df = self.load_previous_data()
            
            # 3. ƒê·ªçc d·ªØ li·ªáu Excel m·ªõi
            raw_df = self.read_excel_data(source_file)
            
            # 4. X·ª≠ l√Ω d·ªØ li·ªáu
            processed_df = self.process_data(raw_df)
            
            # 5. Validation
            is_valid, warnings = self.validate_data(processed_df, previous_df)
            
            if not is_valid and not force:
                logger.error("‚ùå D·ªØ li·ªáu kh√¥ng pass validation. S·ª≠ d·ª•ng --force ƒë·ªÉ b·ªè qua.")
                return False
            
            # 6. L∆∞u file CSV
            if not self.save_csv(processed_df):
                return False
            
            # Note: PE Forward calculation is now integrated into daily valuation update
            # Run: python3 data_processor/valuation/run_optimized_daily_update.py
            
            # 7. T·∫°o b√°o c√°o
            processing_time = (datetime.now() - start_time).total_seconds()
            report = self.create_processing_report(processed_df, warnings, processing_time)
            
            # 8. Th√¥ng b√°o k·∫øt qu·∫£
            logger.info("="*60)
            logger.info("üéâ HO√ÄN TH√ÄNH BSC AUTO UPDATE!")
            logger.info("="*60)
            logger.info(f"üìä ƒê√£ x·ª≠ l√Ω: {report['records_processed']} records")
            logger.info(f"üè¢ S·ªë symbols: {report['symbols_count']}")
            logger.info(f"‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω: {report['processing_time_seconds']}s")
            logger.info(f"üìÅ File output: {self.latest_csv}")
            
            if warnings:
                logger.info(f"‚ö†Ô∏è C√≥ {len(warnings)} c·∫£nh b√°o - xem log ƒë·ªÉ bi·∫øt chi ti·∫øt")
            
            logger.info("‚úÖ Streamlit dashboard s·∫Ω t·ª± ƒë·ªông load d·ªØ li·ªáu m·ªõi!")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói trong qu√° tr√¨nh auto update: {e}")
            return False


def main():
    """Main function v·ªõi command line arguments"""
    parser = argparse.ArgumentParser(description='BSC Forecast Auto Update System')
    parser.add_argument('--source', '-s', help='ƒê∆∞·ªùng d·∫´n file Excel ngu·ªìn')
    parser.add_argument('--force', '-f', action='store_true', help='B·ªè qua validation errors')
    parser.add_argument('--config', '-c', default='data_processor/forecast/bsc_config.json', 
                       help='ƒê∆∞·ªùng d·∫´n file c·∫•u h√¨nh')
    
    args = parser.parse_args()
    
    try:
        # Kh·ªüi t·∫°o processor
        processor = BSCAutoProcessor(config_path=args.config)
        
        # Ch·∫°y quy tr√¨nh
        success = processor.run(source_file=args.source, force=args.force)
        
        if success:
            print("\nüéâ BSC Auto Update ho√†n th√†nh th√†nh c√¥ng!")
            print(f"üìÅ File CSV m·ªõi: {processor.latest_csv}")
            print("üí° C√≥ th·ªÉ s·ª≠ d·ª•ng ngay trong Streamlit dashboard.")
            sys.exit(0)
        else:
            print("\n‚ùå BSC Auto Update th·∫•t b·∫°i!")
            print("üîç Ki·ªÉm tra log ƒë·ªÉ bi·∫øt chi ti·∫øt l·ªói.")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è ƒê√£ d·ª´ng qu√° tr√¨nh theo y√™u c·∫ßu ng∆∞·ªùi d√πng.")
        sys.exit(1)
    except Exception as e:
        print(f"\nüí• L·ªói kh√¥ng mong mu·ªën: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
