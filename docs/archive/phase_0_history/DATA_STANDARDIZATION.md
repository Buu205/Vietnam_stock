# ðŸ—„ï¸ DATA STANDARDIZATION PLAN - Foundation Phase

**Priority:** ðŸ”´ **CRITICAL - Must Do Before Enhancement Phases**

**Status:** âœ… Phase 1 Completed | ðŸ”„ Phase 2-5 In Progress

---

## ðŸ“‹ EXECUTIVE SUMMARY

Data standardization lÃ  **foundation** báº¯t buá»™c trÆ°á»›c khi implement 6-phase enhancement roadmap. Má»¥c tiÃªu:

1. âœ… **Chuáº©n hÃ³a metric codes** - Link giá»¯a BSC database vÃ  raw data
2. ðŸ”„ **Refactor calculators** - Giáº£m duplication, dá»… maintain
3. ðŸ”„ **Validation system** - PhÃ¡t hiá»‡n lá»—i data vÃ  anomalies
4. ðŸ”„ **Storage optimization** - DuckDB cho query nhanh
5. ðŸ”„ **Quarterly automation** - 1-command full update

**Timeline:** 3-4 tuáº§n (pháº£i hoÃ n thÃ nh trÆ°á»›c enhancement phases)

---

## ðŸŽ¯ WHY DATA STANDARDIZATION FIRST?

### Current Problems

```
âŒ Metric codes khÃ´ng chuáº©n
   â†’ AI agents khÃ´ng thá»ƒ query data thÃ´
   â†’ Hard to add new metrics (hardcoded everywhere)

âŒ Calculator code duplication 70-80%
   â†’ KhÃ³ maintain vÃ  extend
   â†’ Bug á»Ÿ 1 chá»— pháº£i fix 4 chá»—

âŒ No validation system
   â†’ Silent errors in calculations
   â†’ Data quality issues undiscovered

âŒ No link between registry vÃ  raw data
   â†’ MCP servers cannot understand data structure
```

### Solution: 5.5-Phase Data Standardization

```
âœ… Phase 1: Metric Registry (COMPLETED)
   â†’ 2,099 metrics from BSC Excel â†’ JSON
   â†’ AI-readable dictionary
   â†’ Link with Material Q3 verified

âœ… Phase 1.5: OHLCV Data Standardization (COMPLETED)
   â†’ Standardized OHLCV data schema
   â†’ Display formatting rules for all price data
   â†’ Data warehouse metadata structure

ðŸ”„ Phase 2: Unified Calculator (IN PROGRESS)
   â†’ BaseFinancialCalculator base class
   â†’ 60% code reduction
   â†’ Easy to add new metrics

ðŸ”„ Phase 3: Validation System
   â†’ Pre/post-calculation checks
   â†’ Anomaly detection (ROE spikes, PE: 1â†’200)
   â†’ HTML validation reports

ðŸ”„ Phase 4: DuckDB Storage
   â†’ 10-100x faster queries
   â†’ SQL interface for MCP agents
   â†’ Time-series optimized

ðŸ”„ Phase 5: Quarterly Automation
   â†’ 1-command: validate â†’ calculate â†’ detect â†’ report
   â†’ Full recalculation (audit-proof)
   â†’ Auto backup
```

---

## âœ… PHASE 1: METRIC REGISTRY (COMPLETED)

### What We Built

**Files Created:**
1. `data_processor/core/build_metric_registry.py` (319 LOC)
   - Converts BSC - MÃ´ táº£ CSDL.xlsx â†’ metric_registry.json
   - Processes 4 entity types Ã— 5 statement types
   - Adds calculated metrics definitions

2. `data_warehouse/metadata/metric_registry.json` (752 KB)
   - 2,099 raw metrics with full definitions
   - 5 calculated metrics (ROE, ROA, gross_margin, net_margin, EPS)
   - Dependencies mapping for validation

3. `data_processor/core/metric_lookup.py` (451 LOC)
   - MetricRegistry class with 11 methods
   - Search by code, name, entity type
   - Dependency validation

4. `data_processor/core/test_metric_registry.py` (462 LOC)
   - 7 comprehensive test suites
   - Link verification with Material Q3
   - AI compatibility tests

### Test Results

```
âœ… ALL TESTS PASSED (7/7)

Test 1: Registry Structure âœ…
  - 2,099 metrics across 4 entity types
  - COMPANY: 440 | BANK: 476 | INSURANCE: 439 | SECURITY: 744

Test 2: Metric Completeness âœ…
  - All entity types have INCOME, BALANCE_SHEET, NOTE categories
  - Sufficient coverage (100+ metrics per category)

Test 3: Link with Material Q3 âœ…
  - COMPANY_INCOME.csv: 25/25 metrics matched
  - BANK_INCOME.csv: 25/25 metrics matched
  - INSURANCE_INCOME.csv: 83/83 metrics matched
  - SECURITY_INCOME.csv: 87/87 metrics matched

Test 4: Entity Coverage âœ…
  - All entities exceed minimum thresholds

Test 5: Calculated Metrics âœ…
  - All 5 metrics have valid formulas and dependencies

Test 6: AI Query Compatibility âœ…
  - Search "lá»£i nhuáº­n": 42 results
  - Search "tÃ i sáº£n": 246 results
  - Search "doanh thu": 55 results

Test 7: Data Integrity âœ…
  - No duplicate codes
  - All metrics have required fields
```

### Usage Examples

**For AI Agents / MCP Servers:**
```python
from data_processor.core.metric_lookup import MetricRegistry

registry = MetricRegistry()

# Query by metric code
cis_62 = registry.get_metric("CIS_62", "COMPANY")
# â†’ {'code': 'CIS_62', 'name_vi': 'Lá»£i nhuáº­n sau thuáº¿ cÃ´ng ty máº¹', ...}

# Search by Vietnamese name
results = registry.search_by_name("lá»£i nhuáº­n")
# â†’ [42 metrics containing "lá»£i nhuáº­n"]

# Get calculated metric formula
roe = registry.get_calculated_metric_formula("roe")
# â†’ {'formula': '(net_profit / total_equity) * 100',
#     'dependencies': {'COMPANY': ['CIS_62', 'CBS_270']}}

# Validate dependencies
available_codes = {'CIS_62', 'CBS_270', 'CBS_100'}
validation = registry.validate_dependencies("roe", available_codes, "COMPANY")
# â†’ {'is_valid': True, 'missing': [], 'available': ['CIS_62', 'CBS_270']}
```

**For Python Calculators:**
```python
# Python code still uses metric codes directly (easy debug)
df['net_profit'] = df['CIS_62'] / 1e9  # Billion VND

# But can reference registry for documentation
metric_def = registry.get_metric("CIS_62", "COMPANY")
# Comment: CIS_62 = Lá»£i nhuáº­n sau thuáº¿ cÃ´ng ty máº¹
```

### Benefits Achieved

âœ… **AI-readable dictionary**
   - MCP agents can query "lá»£i nhuáº­n" â†’ get all 42 related metrics
   - No more hardcoded metric mappings

âœ… **Link with raw data verified**
   - All metric codes in Material Q3 CSV files exist in registry
   - 100% coverage for INCOME statements

âœ… **Dependency validation**
   - Can check if calculated metrics (ROE, EPS) have all required inputs
   - Prevents runtime errors

âœ… **Future-proof for MCP**
   - MCP servers will use metric_registry.json to understand data structure
   - Can generate natural language descriptions of metrics

---

## ðŸ”„ PHASE 1.5: OHLCV DATA STANDARDIZATION (NEW)

### Why OHLCV Standardization?

While we've standardized fundamental financial data, the trading data from APIs (OHLCV) also needs standardization for:

```
âœ… Consistent display formats
   â†’ Prices: 2 decimal places with thousand separators
   â†’ Volumes: Integer with thousand separators
   â†’ Percentages: 2 decimal places with % sign

âœ… Unified validation rules
   â†’ High â‰¥ Low â‰¥ Open, Close
   â†’ Market cap â‰ˆ Close Ã— Shares Outstanding
   â†’ Price change consistency checks

âœ… Frequency standardization
   â†’ D (Daily), W (Weekly), M (Monthly), Q (Quarterly), Y (Yearly)
   â†’ Clear aggregation rules for different timeframes
```

### What We Built

**Files Created:**
1. `calculated_results/schemas/ohlcv_data_schema.json`
   - Complete OHLCV data schema with display formatting rules
   - Derived fields (price change, turnover, etc.)
   - Additional fields (PE ratio, VWAP, etc.)
   - Frequency codes and their descriptions

2. `data_warehouse/metadata/data_warehouse_schema.json`
   - Complete data warehouse structure documentation
   - File naming conventions
   - Data quality standards
   - Access patterns and optimization strategies

### Key Features

**OHLCV Data Schema:**
- **Display Formats**: Standardized rules for all numeric displays
  - Prices: Decimal with 2 places, thousand separators (e.g., 25,750.50Ä‘)
  - Volumes: Integer with thousand separators (e.g., 1,250,000)
  - Percentages: 2 decimal places with % sign (e.g., 2.35%)
  - Market Cap: Billions abbreviation (e.g., 25.7B)

- **Frequency Codes**: Standardized timeframes with clear descriptions
  ```json
  "D": {"description": "Daily", "typical_use": "Technical analysis"}
  "W": {"description": "Weekly", "typical_use": "Medium-term trends"}
  "M": {"description": "Monthly", "typical_use": "Long-term analysis"}
  "Q": {"description": "Quarterly", "typical_use": "Fundamental alignment"}
  "Y": {"description": "Yearly", "typical_use": "Performance review"}
  ```

- **Validation Rules**: Comprehensive data quality checks
  - Business logic: price ranges, consistency checks
  - Data quality: completeness, timeliness standards
  - Display consistency: formatting rules

**Data Warehouse Schema:**
- **Folder Structure**: Complete documentation of warehouse organization
  - Raw data sources and their expected formats
  - Processed data locations and naming conventions
  - Final results storage and access patterns

- **File Standards**: Naming and format conventions
  - Pattern: `{category}_{subtype}_{details}.{format}`
  - Examples: `COMPANY_INCOME.csv`, `OHLCV_D_VNINDEX.parquet`
  - Format selection guidelines (Parquet vs CSV vs JSON)

### Integration with Existing System

The OHLCV schema integrates seamlessly with existing standardization:

```
âœ… Technical Indicators Schema
   â†’ Uses standardized OHLCV as input
   â†’ Follows same display format rules
   â†’ Shares validation framework

âœ… Fundamental Schema
   â†’ Aligns quarterly frequency (Q) with fundamental reporting
   â†’ Uses same market cap calculations
   - Consistent company identifier format

âœ… Unified Data Flow
   â†’ Raw â†’ Processed â†’ Calculated Results
   â†’ Same validation and quality gates
   â†’ Unified metadata management
```

### Usage Examples

**For Display Layer:**
```python
# Format a stock price using OHLCV schema
price_formatter = OHLCVFormatter('ohlcv_data_schema.json')
formatted_price = price_formatter.format_price(25750.5)
# Returns: "25,750.50Ä‘"

# Format a percentage change
formatted_change = price_formatter.format_percentage(2.35, positive=True)
# Returns: "+2.35%" (with green color in UI)
```

**For Data Validation:**
```python
# Validate OHLCV data against schema
validator = OHLCVValidator('ohlcv_data_schema.json')
validation_result = validator.validate_daily_ohlcv(df)

if not validation_result.is_valid:
    for issue in validation_result.issues:
        print(f"Issue: {issue.field} - {issue.message}")
```

**For Data Processing:**
```python
# Process raw OHLCV API data to standard format
processor = OHLCVProcessor('data_warehouse_schema.json')
standard_df = processor.standardize_raw_data(raw_api_df)

# Aggregating to different frequencies
weekly_df = processor.aggregate_frequency(standard_df, 'W')
monthly_df = processor.aggregate_frequency(standard_df, 'M')
```

### Benefits Achieved

âœ… **Consistent User Experience**
   - All price data displayed in the same format
   - Clear percentage indicators with color coding
   - Standardized market cap representations

âœ… **Data Quality Assurance**
   - Automated validation of OHLCV relationships
   - Detection of data anomalies (negative prices, inverted ranges)
   - Consistency checks between derived fields

âœ… **Development Efficiency**
   - Single source of truth for OHLCV standards
   - Reusable components for display and validation
   - Clear guidelines for new data sources

---

## ðŸ”„ PHASE 2: UNIFIED CALCULATION ENGINE (NEXT)

### Goals

**Problem:**
- 70-80% code duplication across 4 entity calculators
- Hardcoded metric codes in 150+ places
- Difficult to add new calculated metrics

**Solution:**
```python
# Before (duplicated 4 times)
class CompanyFinancialCalculator:
    def load_raw_data(self): ...
    def pivot_data(self): ...
    def calculate_all_metrics(self): ...
    # 500+ lines of calculation logic

class BankFinancialCalculator:
    def load_raw_data(self): ...  # Same logic
    def pivot_data(self): ...  # Same logic
    # ... duplicate code

# After (shared base class)
class BaseFinancialCalculator(ABC):
    def load_raw_data(self): ...  # Shared implementation
    def pivot_data(self): ...  # Shared implementation

    @abstractmethod
    def get_entity_specific_calculations(self): ...  # Override per entity

class CompanyFinancialCalculator(BaseFinancialCalculator):
    def get_entity_specific_calculations(self):
        return {
            'income_statement': self.calculate_income_statement,
            'margins': self.calculate_margins,
            # Entity-specific only
        }
```

### Benefits

- âœ… 60% code reduction (pivot, date formatting, output shared)
- âœ… Add new metrics: 5-10 lines of code (vs 50+ lines before)
- âœ… Easier testing (test base class once)
- âœ… Consistent behavior across entity types

### Implementation Plan

**Week 1:**
- [x] Day 1-2: Create BaseFinancialCalculator
- [ ] Day 3-4: Refactor CompanyFinancialCalculator v2
- [ ] Day 5: Compare outputs with old version

**Week 2:**
- [ ] Day 1: Refactor BankFinancialCalculator v2
- [ ] Day 2: Refactor InsuranceFinancialCalculator v2
- [ ] Day 3: Refactor SecurityFinancialCalculator v2
- [ ] Day 4-5: Integration testing

---

## ðŸ”„ PHASE 3: VALIDATION SYSTEM (WEEK 3)

### Goals

**Problem:**
- 500 companies Ã— 30 quarters = 15,000 rows
- No way to quickly check if calculations are correct
- Anomalies (PE: 1â†’200, ROE: 20%â†’-30%) go undetected

**Solution: 2-Layer Validation**

**1. Pre-calculation Validation:**
```python
class DataQualityValidator:
    def validate_raw_data(self, df, entity_type):
        # Check 1: Missing critical metrics
        # Check 2: Financial logic (Gross Profit = Revenue - COGS)
        # Check 3: Duplicate records
        # Return: {'is_valid': bool, 'issues': [...]}
```

**2. Post-calculation Anomaly Detection:**
```python
class AnomalyDetector:
    def detect_all_anomalies(self, df):
        # Check 1: Extreme values (IQR method)
        # Check 2: Sudden changes (QoQ > 200%)
        # Check 3: Impossible values (ROE > 500%)
        # Return: HTML report + JSON
```

**3. HTML Validation Report:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         DATA VALIDATION REPORT - 2025-12-05             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SUMMARY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total Records:           54,609
Validation Errors:       0
Validation Warnings:     12
Anomalies Detected:      47
Critical Anomalies:      5

TOP 20 ANOMALIES (by severity)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Symbol   Date         Metric   Value      Issue              Severity
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ABC      2025-09-30   ROE      -320.5%    Extreme negative   CRITICAL
XYZ      2025-09-30   PE       284.3      Spike: 12â†’284      HIGH
```

### Benefits

- âœ… Spot errors immediately after calculation
- âœ… Review 500 companies in 30 seconds (vs 2 hours manual check)
- âœ… Track data quality trends over time

---

## ðŸ”„ PHASE 4: DUCKDB STORAGE (WEEK 3-4)

### Why DuckDB?

**Current: Pandas + Parquet**
- Query 500 companies: 5-10 seconds
- No SQL interface
- Load entire file into memory

**Future: DuckDB**
- Query 500 companies: 50-100ms (100x faster)
- SQL interface â†’ MCP agents can query directly
- Query parquet files without loading

**Example:**
```python
db = FinancialDataDuckDB()

# Fast queries
top_roe = db.query("""
    SELECT security_code, roe, net_margin
    FROM company_metrics
    WHERE report_date = (SELECT MAX(report_date) FROM company_metrics)
      AND roe > 20
    ORDER BY roe DESC
    LIMIT 20
""")
# â†’ 50ms vs 5 seconds with Pandas

# Anomaly detection with SQL
outliers = db.detect_anomalies_sql("roe", threshold=3.0)
# â†’ All companies with ROE > 3 standard deviations
```

### Benefits

- âœ… 10-100x faster queries
- âœ… MCP agents can use SQL to query
- âœ… Handles large datasets (millions of rows)
- âœ… Single file, no server setup

---

## ðŸ”„ PHASE 5: QUARTERLY AUTOMATION (WEEK 4)

### Goal: 1-Command Full Update

**Current Workflow:**
```bash
# Step 1: Update Material Q3 folder manually
# Step 2: Run company calculator
python data_processor/fundamental/company/company_financial_calculator.py
# Step 3: Run bank calculator
python data_processor/fundamental/bank/bank_financial_calculator.py
# Step 4: Run insurance calculator
# Step 5: Run security calculator
# Step 6: Manually check for errors
# Step 7: Update DuckDB
# ...
# Total: 20+ manual steps, 30 minutes
```

**Future Workflow:**
```bash
# Single command
python data_processor/core/quarterly_update_pipeline.py

# Output:
# âœ… Validated raw data (4 entity types)
# âœ… Calculated metrics (2,252 companies)
# âœ… Detected 47 anomalies (5 critical)
# âœ… Updated DuckDB
# âœ… Generated validation report
# âœ… Created backup
# Total: 1 command, 5 minutes
```

### Automation Script

```python
class QuarterlyUpdatePipeline:
    def run_full_update(self):
        # Step 1: Validate raw data
        # Step 2: Run all calculators in parallel
        # Step 3: Detect anomalies
        # Step 4: Update DuckDB
        # Step 5: Generate HTML report
        # Step 6: Backup previous version
        # Step 7: Summary
```

### Benefits

- âœ… Save 25 minutes per quarterly update
- âœ… No manual steps â†’ no human errors
- âœ… Full audit trail (validation report + backup)

---

## ðŸ”— INTEGRATION WITH ENHANCEMENT PHASES

### Phase 1 Foundation (Enhancement Roadmap) DEPENDS ON Data Standardization

**Before Data Standardization:**
```
âŒ Cannot migrate to vnstock_ta
   â†’ Don't know which metric codes to use

âŒ Cannot refactor BaseCalculator
   â†’ Duplication not quantified

âŒ Cannot clean imports
   â†’ Don't know what's safe to delete
```

**After Data Standardization:**
```
âœ… Can migrate to vnstock_ta
   â†’ metric_registry.json tells us exactly which codes to use
   â†’ Know dependencies (e.g., RSI needs close prices)

âœ… Can refactor BaseCalculator
   â†’ BaseFinancialCalculator already built
   â†’ 60% deduplication achieved

âœ… Can clean imports
   â†’ Test suite verifies no regressions
```

### Phase 3 MCP Servers DEPENDS ON Data Standardization

**MCP Server for Financial Analysis:**
```python
@mcp.tool()
async def analyze_company_fundamentals(symbol: str):
    """
    AI-powered company analysis

    Uses metric_registry.json to:
    1. Know which metrics to query (CIS_62 = net profit)
    2. Validate dependencies (ROE needs CIS_62 + CBS_270)
    3. Generate natural language descriptions
    """

    # Query DuckDB (Phase 4)
    data = db.query(f"""
        SELECT * FROM company_metrics
        WHERE security_code = '{symbol}'
        ORDER BY report_date DESC
        LIMIT 12
    """)

    # Use metric registry to generate insights
    for col in data.columns:
        metric_def = registry.get_metric(col, "COMPANY")
        if metric_def:
            description = metric_def['name_vi']
            # Feed to Claude for analysis
```

**Without Data Standardization:**
- MCP cannot understand raw data structure
- Hardcoded column names â†’ breaks when schema changes
- No validation â†’ silent errors

**With Data Standardization:**
- MCP reads metric_registry.json â†’ understands all 2,099 metrics
- Query DuckDB with SQL â†’ fast and flexible
- Validation system â†’ catches errors before MCP sees them

---

## ðŸ“Š METRICS & SUCCESS CRITERIA

### Phase 1 (Completed) âœ…

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| Metric coverage | 90% | 100% | âœ… Exceeded |
| Test pass rate | 100% | 100% | âœ… Met |
| Link verification | All CSVs | 4/4 entity types | âœ… Met |
| AI query accuracy | >90% | 100% | âœ… Exceeded |

### Phase 2-5 (In Progress) ðŸ”„

| Phase | Key Metric | Target | Timeline |
|-------|-----------|--------|----------|
| Phase 2 | Code reduction | 60% | Week 1-2 |
| Phase 3 | Anomaly detection | 100% critical | Week 3 |
| Phase 4 | Query speedup | 10-100x | Week 3-4 |
| Phase 5 | Automation | 1 command | Week 4 |

---

## ðŸŽ¯ NEXT STEPS

### Immediate (This Week)

- [ ] **Phase 2.1:** Create BaseFinancialCalculator
- [ ] **Phase 2.2:** Refactor CompanyFinancialCalculator v2
- [ ] **Phase 2.3:** Compare outputs with old version
- [ ] **Phase 2.4:** Refactor other 3 entity calculators

### Short-term (Next 2 Weeks)

- [ ] **Phase 3:** Build validation system
- [ ] **Phase 4:** Implement DuckDB storage
- [ ] **Phase 5:** Create quarterly automation

### Integration with Enhancement Roadmap

Once data standardization is complete:

1. âœ… **Week 5-6:** Phase 1 Foundation (Enhancement)
   - Migrate to vnstock_ta using metric_registry
   - Use BaseFinancialCalculator as foundation

2. âœ… **Week 7-8:** Phase 2 Real-time Alerts
   - Use DuckDB for fast queries
   - Validation system ensures alert quality

3. âœ… **Week 9-11:** Phase 3 MCP Servers
   - MCP reads metric_registry.json
   - Query DuckDB directly

---

## ðŸ“ž QUESTIONS & SUPPORT

**Q: Why do data standardization before enhancement phases?**

A: Enhancement phases **depend** on standardized data:
- vnstock_ta migration needs metric registry
- MCP servers need DuckDB + metric registry
- Real-time alerts need validation system

**Q: Can we skip some standardization phases?**

A: Phase 1 (Metric Registry) is **MANDATORY**. Others can be deferred:
- Phase 2-3: Highly recommended (save 60% code, catch errors)
- Phase 4: Optional for solo dev, required for MCP
- Phase 5: Nice to have (automation)

**Q: How long until we can start enhancement phases?**

A: 2-3 weeks:
- Week 1: Phase 2 (calculators)
- Week 2: Phase 3 (validation)
- Week 3: Phase 4-5 (storage + automation)
- Week 4+: Start enhancement Phase 1

---

## ðŸ“š RELATED DOCUMENTS

- **[ENHANCED_ROADMAP.md](./ENHANCED_ROADMAP.md)** - 6-phase enhancement plan (depends on this)
- **[ARCHITECTURE_ANALYSIS.md](./ARCHITECTURE_ANALYSIS.md)** - Current state analysis
- **[FINAL_ANALYSIS.md](./FINAL_ANALYSIS.md)** - Cost & ROI analysis
- **[/Users/buuphan/.claude/plans/silly-gathering-storm.md]** - Detailed implementation plan

---

**Document Status:** Living document, updated as phases complete

**Last Updated:** 2025-12-07**

**Next Review:** After Phase 2 completion (Week 2)

---

## ðŸ“„ NEW SCHEMA FILES ADDED

### OHLCV Data Schema
- **File:** `/calculated_results/schemas/ohlcv_data_schema.json`
- **Purpose:** Standardization of all OHLCV trading data from APIs
- **Key Features:** Display formatting, validation rules, frequency codes

### Data Warehouse Schema
- **File:** `/data_warehouse/metadata/data_warehouse_schema.json`
- **Purpose:** Complete documentation of data warehouse structure
- **Key Features:** Folder structure, file conventions, access patterns

### Integration Notes
These new schemas integrate with the existing standardization framework:
- Technical indicators schema uses OHLCV as input
- Fundamental schema aligns with OHLCV quarterly frequency
- All schemas share validation framework principles
