# üìã PHASE 0.3 - DETAILED CLEANUP GUIDE
## Chi ti·∫øt t·ª´ng th∆∞ m·ª•c - X√≥a g√¨, Gi·ªØ g√¨, Test th·∫ø n√†o

**Created:** 2025-12-07
**Status:** üî¥ **ACTION REQUIRED - Follow Step by Step**

---

## üìä CURRENT STATE - What We Have Now

### ‚úÖ NEW Structure (v3.0) - KEEP THESE
```
DATA/                   1.1GB    ‚Üê NEW: All data centralized
‚îú‚îÄ‚îÄ raw/               253MB    ‚Üê FROM: data_warehouse/raw/
‚îú‚îÄ‚îÄ processed/         834MB    ‚Üê FROM: calculated_results/
‚îú‚îÄ‚îÄ metadata/          864KB    ‚Üê FROM: data_warehouse/metadata/
‚îî‚îÄ‚îÄ schemas/           100KB    ‚Üê FROM: calculated_results/schemas/ + data_warehouse/schemas/

PROCESSORS/            9.9MB    ‚Üê NEW: All processing logic
‚îú‚îÄ‚îÄ core/                       ‚Üê FROM: data_processor/core/
‚îú‚îÄ‚îÄ fundamental/                ‚Üê FROM: data_processor/fundamental/base/
‚îú‚îÄ‚îÄ technical/                  ‚Üê FROM: data_processor/technical/
‚îú‚îÄ‚îÄ valuation/                  ‚Üê FROM: data_processor/valuation/
‚îú‚îÄ‚îÄ news/                       ‚Üê FROM: data_processor/news/
‚îî‚îÄ‚îÄ forecast/                   ‚Üê FROM: data_processor/Bsc_forecast/

WEBAPP/                         ‚Üê FROM: streamlit_app/ (renamed)
CONFIG/                         ‚Üê KEEP (already clean)
logs/                           ‚Üê KEEP (centralized)
archive/                        ‚Üê KEEP (v1.0 deprecated code)
```

### ‚ùå OLD Structure (v2.0) - DELETE THESE
```
data_warehouse/        335MB    ‚ùå DELETE (duplicated in DATA/)
calculated_results/    834MB    ‚ùå DELETE (duplicated in DATA/processed/)
data_processor/        9.9MB    ‚ùå DELETE (duplicated in PROCESSORS/)
streamlit_app/                  ‚ùå DELETE (renamed to WEBAPP/)
mcp_server/                     ‚ùå DELETE (renamed to MCP/ if exists)
```

---

## üîç DETAILED DIRECTORY AUDIT

### 1. data_warehouse/ (335MB) - CAN DELETE

**What's inside:**
```bash
data_warehouse/
‚îú‚îÄ‚îÄ raw/                     ‚úÖ COPIED ‚Üí DATA/raw/
‚îÇ   ‚îú‚îÄ‚îÄ ohlcv/              (OHLCV_mktcap.parquet - 164MB)
‚îÇ   ‚îú‚îÄ‚îÄ fundamental/        (Material Q3 CSVs)
‚îÇ   ‚îú‚îÄ‚îÄ commodity/
‚îÇ   ‚îú‚îÄ‚îÄ macro/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ metadata/                ‚úÖ COPIED ‚Üí DATA/metadata/
‚îÇ   ‚îú‚îÄ‚îÄ metric_registry.json
‚îÇ   ‚îú‚îÄ‚îÄ sector_industry_registry.json
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ schemas/                 ‚úÖ MERGED ‚Üí DATA/schemas/
‚îÇ   ‚îú‚îÄ‚îÄ ohlcv_schema.json   (merged with ohlcv_data_schema.json)
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ cache/                   ‚ö†Ô∏è OPTIONAL (can regenerate)
```

**Verification Command:**
```bash
# Verify all raw data copied
diff -r data_warehouse/raw/ DATA/raw/ --brief

# Verify all metadata copied
diff -r data_warehouse/metadata/ DATA/metadata/ --brief
```

**Safe to Delete:** ‚úÖ YES (after verification)

---

### 2. calculated_results/ (834MB) - CAN DELETE

**What's inside:**
```bash
calculated_results/
‚îú‚îÄ‚îÄ fundamental/             ‚úÖ COPIED ‚Üí DATA/processed/fundamental/
‚îÇ   ‚îú‚îÄ‚îÄ company/            (company_financial_metrics.parquet)
‚îÇ   ‚îú‚îÄ‚îÄ bank/               (bank_financial_metrics.parquet)
‚îÇ   ‚îú‚îÄ‚îÄ insurance/          (insurance_financial_metrics.parquet)
‚îÇ   ‚îî‚îÄ‚îÄ security/           (security_financial_metrics.parquet)
‚îú‚îÄ‚îÄ technical/               ‚úÖ COPIED ‚Üí DATA/processed/technical/
‚îÇ   ‚îú‚îÄ‚îÄ basic_data.parquet
‚îÇ   ‚îú‚îÄ‚îÄ moving_averages.parquet
‚îÇ   ‚îú‚îÄ‚îÄ rsi.parquet
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ valuation/               ‚úÖ COPIED ‚Üí DATA/processed/valuation/
‚îÇ   ‚îú‚îÄ‚îÄ stock_pe_pb.parquet
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ commodity/               ‚úÖ COPIED ‚Üí DATA/processed/commodity/
‚îú‚îÄ‚îÄ macro/                   ‚úÖ COPIED ‚Üí DATA/processed/macro/
‚îî‚îÄ‚îÄ schemas/                 ‚úÖ MERGED ‚Üí DATA/schemas/
    ‚îú‚îÄ‚îÄ ohlcv_data_schema.json
    ‚îú‚îÄ‚îÄ fundamental_calculated_schema.json
    ‚îî‚îÄ‚îÄ ...
```

**Verification Command:**
```bash
# Verify all parquet files copied
find calculated_results -name "*.parquet" | wc -l
find DATA/processed -name "*.parquet" | wc -l
# Should be equal (102 files each)

# Check file sizes match
du -sh calculated_results/
du -sh DATA/processed/
# Should be ~834MB each
```

**Safe to Delete:** ‚úÖ YES (after verification)

---

### 3. data_processor/ (9.9MB, 71 Python files) - NEEDS CAREFUL REVIEW

**What's inside:**
```bash
data_processor/
‚îú‚îÄ‚îÄ core/                    ‚úÖ COPIED ‚Üí PROCESSORS/core/shared/
‚îÇ   ‚îú‚îÄ‚îÄ unified_mapper.py
‚îÇ   ‚îú‚îÄ‚îÄ ohlcv_formatter.py  ‚Üí MOVED to PROCESSORS/core/formatters/
‚îÇ   ‚îú‚îÄ‚îÄ metric_lookup.py    ‚Üí MOVED to PROCESSORS/core/registries/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ fundamental/
‚îÇ   ‚îú‚îÄ‚îÄ base/                ‚úÖ COPIED ‚Üí PROCESSORS/fundamental/calculators/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ company_financial_calculator.py ‚Üí company_calculator.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bank_financial_calculator.py ‚Üí bank_calculator.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ company/             ‚ö†Ô∏è OLD VERSION (archived)
‚îÇ   ‚îú‚îÄ‚îÄ bank/                ‚ö†Ô∏è OLD VERSION (archived)
‚îÇ   ‚îú‚îÄ‚îÄ insurance/           ‚ö†Ô∏è OLD VERSION (archived)
‚îÇ   ‚îî‚îÄ‚îÄ security/            ‚ö†Ô∏è OLD VERSION (archived)
‚îú‚îÄ‚îÄ technical/
‚îÇ   ‚îú‚îÄ‚îÄ ohlcv/               ‚úÖ COPIED ‚Üí PROCESSORS/technical/ohlcv/
‚îÇ   ‚îú‚îÄ‚îÄ indicators/          ‚úÖ COPIED ‚Üí PROCESSORS/technical/indicators/
‚îÇ   ‚îú‚îÄ‚îÄ commodity/           ‚úÖ COPIED ‚Üí PROCESSORS/technical/commodity/
‚îÇ   ‚îú‚îÄ‚îÄ macro/               ‚úÖ COPIED ‚Üí PROCESSORS/technical/macro/
‚îÇ   ‚îú‚îÄ‚îÄ daily_*.py           ‚úÖ COPIED ‚Üí PROCESSORS/technical/pipelines/
‚îÇ   ‚îî‚îÄ‚îÄ technical/           ‚ö†Ô∏è OLD NESTED (empty after flattening)
‚îú‚îÄ‚îÄ valuation/
‚îÇ   ‚îú‚îÄ‚îÄ core/                ‚úÖ COPIED ‚Üí PROCESSORS/valuation/calculators/
‚îÇ   ‚îî‚îÄ‚îÄ daily_full_valuation_pipeline.py ‚úÖ COPIED ‚Üí PROCESSORS/valuation/pipelines/
‚îú‚îÄ‚îÄ news/                    ‚úÖ COPIED ‚Üí PROCESSORS/news/
‚îî‚îÄ‚îÄ Bsc_forecast/            ‚úÖ COPIED ‚Üí PROCESSORS/forecast/
```

**‚ö†Ô∏è CRITICAL FILES - DO NOT DELETE YET:**
```bash
data_processor/fundamental/base/
# These are Phase 0.2 calculators - ALREADY COPIED to PROCESSORS/
# But let's verify they work from new location first
```

**Verification Steps:**

**Step 1: Test PROCESSORS imports work**
```bash
# Test core imports
python3 -c "from PROCESSORS.core.shared.unified_mapper import UnifiedTickerMapper; print('‚úÖ Core works')"

# Test formatters
python3 -c "from PROCESSORS.core.formatters.ohlcv_formatter import OHLCVFormatter; print('‚úÖ Formatters work')"

# Test registries
python3 -c "from PROCESSORS.core.registries.metric_lookup import MetricRegistry; print('‚úÖ Registries work')"
```

**Step 2: Test fundamental calculators**
```bash
# Test company calculator
python3 -c "from PROCESSORS.fundamental.calculators.company_calculator import CompanyFinancialCalculator; print('‚úÖ Company calculator works')"

# Test bank calculator
python3 -c "from PROCESSORS.fundamental.calculators.bank_calculator import BankFinancialCalculator; print('‚úÖ Bank calculator works')"
```

**Step 3: Test technical processors**
```bash
# Test technical processor
python3 -c "from PROCESSORS.technical.indicators.technical_processor import TechnicalProcessor; print('‚úÖ Technical processor works')"
```

**Safe to Delete:** ‚ö†Ô∏è ONLY AFTER ALL TESTS PASS

---

### 4. streamlit_app/ - ALREADY RENAMED

**Status:** ‚úÖ RENAMED to WEBAPP/
```bash
ls -d WEBAPP/  # Should exist
ls -d streamlit_app/  # Should NOT exist (already renamed)
```

**No action needed** - already handled

---

### 5. mcp_server/ - CHECK IF EXISTS

**Status:** May or may not exist
```bash
ls -d mcp_server/ 2>/dev/null || echo "Not found (OK)"
ls -d MCP/ 2>/dev/null || echo "Not found (OK)"
```

**Action:** If mcp_server/ exists, rename to MCP/

---

## üéØ STEP-BY-STEP CLEANUP PLAN

### PHASE 1: VERIFICATION (30 minutes)

#### Step 1.1: Verify DATA/ migration
```bash
# Check data sizes match
echo "Checking raw data..."
du -sh data_warehouse/raw
du -sh DATA/raw
# Should be similar (253MB)

echo "Checking processed data..."
du -sh calculated_results/
du -sh DATA/processed/
# Should be similar (834MB)

echo "Checking metadata..."
du -sh data_warehouse/metadata
du -sh DATA/metadata
# Should be similar (864KB)

echo "Counting parquet files..."
find calculated_results -name "*.parquet" | wc -l
find DATA/processed -name "*.parquet" | wc -l
# Should be equal (102 files)
```

**Expected Result:** All sizes match ‚úÖ

#### Step 1.2: Verify PROCESSORS/ migration
```bash
# Test all critical imports
python3 << 'EOF'
try:
    from PROCESSORS.core.shared.unified_mapper import UnifiedTickerMapper
    from PROCESSORS.core.formatters.ohlcv_formatter import OHLCVFormatter
    from PROCESSORS.core.registries.metric_lookup import MetricRegistry
    print("‚úÖ Core imports work")
except Exception as e:
    print(f"‚ùå Core imports failed: {e}")

try:
    from PROCESSORS.fundamental.calculators.company_calculator import CompanyFinancialCalculator
    print("‚úÖ Fundamental imports work")
except Exception as e:
    print(f"‚ùå Fundamental imports failed: {e}")

try:
    from PROCESSORS.technical.indicators.technical_processor import TechnicalProcessor
    print("‚úÖ Technical imports work")
except Exception as e:
    print(f"‚ùå Technical imports failed: {e}")
EOF
```

**Expected Result:** All imports work ‚úÖ

**If imports fail:** Do NOT proceed to deletion. Fix imports first.

---

### PHASE 2: CREATE BACKUP (10 minutes)

**BEFORE deleting anything, create backup:**

```bash
# Create backup tarball
tar -czf backup_old_structure_$(date +%Y%m%d_%H%M%S).tar.gz \
    data_warehouse/ \
    calculated_results/ \
    data_processor/ \
    2>/dev/null

# Verify backup created
ls -lh backup_old_structure_*.tar.gz

# Should see file ~1.1GB (compressed)
```

**Expected Result:** Backup file created ‚úÖ

---

### PHASE 3: SAFE DELETION (15 minutes)

**‚ö†Ô∏è ONLY proceed if Phase 1 & 2 passed!**

#### Step 3.1: Delete data_warehouse/ (335MB)
```bash
# Verify one more time
diff -r data_warehouse/raw/ DATA/raw/ --brief
diff -r data_warehouse/metadata/ DATA/metadata/ --brief

# If no differences, safe to delete
rm -rf data_warehouse/

# Verify deleted
ls -d data_warehouse/ 2>/dev/null && echo "‚ùå Still exists!" || echo "‚úÖ Deleted"
```

#### Step 3.2: Delete calculated_results/ (834MB)
```bash
# Verify parquet count
find calculated_results -name "*.parquet" | wc -l
find DATA/processed -name "*.parquet" | wc -l

# If counts match, safe to delete
rm -rf calculated_results/

# Verify deleted
ls -d calculated_results/ 2>/dev/null && echo "‚ùå Still exists!" || echo "‚úÖ Deleted"
```

#### Step 3.3: Delete data_processor/ (9.9MB)
```bash
# FINAL CHECK: Test imports work from PROCESSORS/
python3 -c "from PROCESSORS.fundamental.calculators.company_calculator import CompanyFinancialCalculator; print('‚úÖ Ready to delete')"

# If test passes, safe to delete
rm -rf data_processor/

# Verify deleted
ls -d data_processor/ 2>/dev/null && echo "‚ùå Still exists!" || echo "‚úÖ Deleted"
```

---

### PHASE 4: VERIFY CLEAN STATE (5 minutes)

```bash
# Check new structure
echo "=== NEW STRUCTURE (v3.0) ==="
ls -d DATA/ PROCESSORS/ WEBAPP/ CONFIG/
echo ""

# Check sizes
echo "=== DATA SIZES ==="
du -sh DATA/raw DATA/processed DATA/metadata
echo ""

# Check old folders deleted
echo "=== OLD FOLDERS (should not exist) ==="
ls -d data_warehouse/ 2>/dev/null && echo "‚ùå data_warehouse still exists" || echo "‚úÖ data_warehouse deleted"
ls -d calculated_results/ 2>/dev/null && echo "‚ùå calculated_results still exists" || echo "‚úÖ calculated_results deleted"
ls -d data_processor/ 2>/dev/null && echo "‚ùå data_processor still exists" || echo "‚úÖ data_processor deleted"
echo ""

# Final test: Import from new structure
python3 -c "from PROCESSORS.core.config.paths import DATA_ROOT, PROCESSORS_ROOT; print(f'‚úÖ Paths work: DATA={DATA_ROOT}')"
```

**Expected Output:**
```
=== NEW STRUCTURE (v3.0) ===
DATA/  PROCESSORS/  WEBAPP/  CONFIG/

=== DATA SIZES ===
253M    DATA/raw
834M    DATA/processed
864K    DATA/metadata

=== OLD FOLDERS (should not exist) ===
‚úÖ data_warehouse deleted
‚úÖ calculated_results deleted
‚úÖ data_processor deleted

‚úÖ Paths work: DATA=.../stock_dashboard/DATA
```

---

## üß™ TESTING PLAN - Verify Everything Works

### Test 1: Paths Configuration
```bash
python3 PROCESSORS/core/config/paths.py
```

**Expected:** Should print all paths correctly

### Test 2: Import New Calculators
```bash
python3 << 'EOF'
from PROCESSORS.fundamental.calculators.company_calculator import CompanyFinancialCalculator
from PROCESSORS.fundamental.calculators.bank_calculator import BankFinancialCalculator
print("‚úÖ All calculators import successfully")
EOF
```

### Test 3: Load Data from New Paths
```bash
python3 << 'EOF'
from PROCESSORS.core.config.paths import DATA_ROOT, PROCESSED_FUNDAMENTAL
import pandas as pd

# Try to load company metrics
company_file = PROCESSED_FUNDAMENTAL / "company" / "company_financial_metrics.parquet"
df = pd.read_parquet(company_file)
print(f"‚úÖ Loaded company metrics: {len(df)} rows, {len(df.columns)} columns")

# Try to load bank metrics
bank_file = PROCESSED_FUNDAMENTAL / "bank" / "bank_financial_metrics.parquet"
df = pd.read_parquet(bank_file)
print(f"‚úÖ Loaded bank metrics: {len(df)} rows, {len(df.columns)} columns")
EOF
```

**Expected:**
```
‚úÖ Loaded company metrics: XXXX rows, XX columns
‚úÖ Loaded bank metrics: XXXX rows, XX columns
```

### Test 4: Run a Simple Calculator (Dry Run)
```bash
# This will test if calculators can access new DATA/ paths
python3 << 'EOF'
from PROCESSORS.fundamental.calculators.company_calculator import CompanyFinancialCalculator
from PROCESSORS.core.config.paths import RAW_FUNDAMENTAL, PROCESSED_FUNDAMENTAL

# Initialize calculator
calc = CompanyFinancialCalculator()

# Check if it can find raw data
print(f"Raw data path: {RAW_FUNDAMENTAL}")
print(f"Output path: {PROCESSED_FUNDAMENTAL}")
print("‚úÖ Calculator initialized successfully")
EOF
```

---

## üö® ROLLBACK PLAN (If Something Goes Wrong)

### If Tests Fail After Deletion

**Step 1: Stop immediately**
```bash
# Do NOT delete more folders if any test fails
```

**Step 2: Restore from backup**
```bash
# Find your backup
ls -lh backup_old_structure_*.tar.gz

# Extract backup
tar -xzf backup_old_structure_YYYYMMDD_HHMMSS.tar.gz

# Verify restored
ls -d data_warehouse/ calculated_results/ data_processor/
```

**Step 3: Report issue**
- Note which test failed
- Check error message
- Review import paths

---

## üìÖ RECOMMENDED EXECUTION SCHEDULE

### Option A: Careful Approach (Recommended)
```
Day 1 Morning:   Phase 1 - Verification (30 min)
Day 1 Afternoon: Phase 2 - Create Backup (10 min)
                 Phase 3 - Delete data_warehouse/ only (5 min)
                 Test everything still works (15 min)

Day 2 Morning:   Phase 3 - Delete calculated_results/ (5 min)
                 Test everything still works (15 min)

Day 2 Afternoon: Phase 3 - Delete data_processor/ (5 min)
                 Phase 4 - Verify clean state (5 min)
                 Full system test (30 min)
```

### Option B: Quick Approach (If confident)
```
Same Day:
1. Phase 1 - Verification (30 min)
2. Phase 2 - Backup (10 min)
3. Phase 3 - Delete all old folders (15 min)
4. Phase 4 - Verify + Test (40 min)

Total: ~2 hours
```

---

## ‚úÖ SUCCESS CRITERIA

After cleanup, you should have:

### Directory Structure
```
‚úÖ DATA/ exists (1.1GB)
   ‚îú‚îÄ‚îÄ raw/ (253MB)
   ‚îú‚îÄ‚îÄ processed/ (834MB)
   ‚îú‚îÄ‚îÄ metadata/ (864KB)
   ‚îî‚îÄ‚îÄ schemas/ (100KB)

‚úÖ PROCESSORS/ exists (9.9MB)
   ‚îú‚îÄ‚îÄ core/
   ‚îú‚îÄ‚îÄ fundamental/
   ‚îú‚îÄ‚îÄ technical/
   ‚îú‚îÄ‚îÄ valuation/
   ‚îú‚îÄ‚îÄ news/
   ‚îî‚îÄ‚îÄ forecast/

‚úÖ WEBAPP/ exists
‚úÖ CONFIG/ exists

‚ùå data_warehouse/ DOES NOT exist
‚ùå calculated_results/ DOES NOT exist
‚ùå data_processor/ DOES NOT exist
```

### Functionality
```
‚úÖ All imports work from PROCESSORS/
‚úÖ Can load data from DATA/
‚úÖ paths.py returns correct paths
‚úÖ Calculators initialize without errors
```

### Backup
```
‚úÖ Backup file exists (~1.1GB compressed)
‚úÖ Can restore from backup if needed
```

---

## üéØ NEXT STEPS AFTER CLEANUP

### Week 2: Formula Extraction
1. Extract formulas from PROCESSORS/fundamental/calculators/
2. Create PROCESSORS/fundamental/formulas/
3. Separate pure calculation logic from data loading

### Week 3: Pipeline Creation
1. Create quarterly_pipeline.py
2. Test automated parquet generation
3. Validation reports

### Week 4: Documentation
1. Update CLAUDE.md
2. Create docs/INDEX.md
3. Migration guide

---

## üìû TROUBLESHOOTING

### Issue: Imports fail after migration
**Solution:** Check PYTHONPATH and sys.path
```python
import sys
print(sys.path)
# Should include /Users/buuphan/Dev/stock_dashboard
```

### Issue: Cannot find DATA/ folder
**Solution:** Check paths.py configuration
```python
from PROCESSORS.core.config.paths import DATA_ROOT
print(DATA_ROOT)
# Should be /Users/buuphan/Dev/stock_dashboard/DATA
```

### Issue: Parquet files not found
**Solution:** Verify migration completed
```bash
find DATA/processed -name "*.parquet" | wc -l
# Should be 102 files
```

---

**Document Status:** üî¥ **READY TO EXECUTE**
**Last Updated:** 2025-12-07
**Next Review:** After Phase 3 completion

---

## üìù EXECUTION CHECKLIST

```
Pre-Cleanup:
[ ] Read this entire document
[ ] Understand each phase
[ ] Set aside 2-3 hours
[ ] Have backup plan ready

Phase 1: Verification
[ ] Check DATA/ sizes match old folders
[ ] Test all PROCESSORS/ imports
[ ] Verify parquet file counts match
[ ] All tests pass ‚úÖ

Phase 2: Backup
[ ] Create backup tarball
[ ] Verify backup file exists (>1GB)
[ ] Test can extract backup

Phase 3: Deletion
[ ] Delete data_warehouse/
[ ] Delete calculated_results/
[ ] Delete data_processor/
[ ] Verify all deleted

Phase 4: Verification
[ ] Check new structure exists
[ ] Test imports from PROCESSORS/
[ ] Test loading data from DATA/
[ ] Run calculator dry run
[ ] All tests pass ‚úÖ

Post-Cleanup:
[ ] Update git status
[ ] Commit changes
[ ] Ready for Week 2 (Formula Extraction)
```

---

**Ready to proceed! Follow steps carefully. üöÄ**
